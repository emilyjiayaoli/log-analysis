{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4181d83-8eef-4a5e-9b2b-9e459ced8e84",
   "metadata": {
    "id": "c4181d83-8eef-4a5e-9b2b-9e459ced8e84"
   },
   "source": [
    "# Building a Live RAG Pipeline over Google Drive Files\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/ingestion/ingestion_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "In this guide we show you how to build a \"live\" RAG pipeline over Google Drive files.\n",
    "\n",
    "This pipeline will index Google Drive files and dump them to a Redis vector store. Afterwards, every time you rerun the ingestion pipeline, the pipeline will propagate **incremental updates**, so that only changed documents are updated in the vector store. This means that we don't re-index all the documents!\n",
    "\n",
    "We use the following [data source](https://drive.google.com/drive/folders/1RFhr3-KmOZCR5rtp4dlOMNl3LKe1kOA5?usp=sharing) - you will need to copy these files and upload them to your own Google Drive directory!\n",
    "\n",
    "**NOTE**: You will also need to setup a service account and credentials.json. See our LlamaHub page for the Google Drive loader for more details: https://llamahub.ai/l/readers/llama-index-readers-google?from=readers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7caa90-8418-4b1b-8dc4-31ac81da39f3",
   "metadata": {
    "id": "4a7caa90-8418-4b1b-8dc4-31ac81da39f3"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We install required packages and launch the Redis Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d9bdd-1684-421f-ab6d-69112b652f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama_index\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5179ede",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5179ede",
    "outputId": "4c560245-7a75-40f9-c30e-c6b8d20373ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-storage-docstore-redis\n",
    "%pip install llama-index-vector-stores-redis\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-readers-google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f480f0-71e4-4d50-8efa-deae20172764",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03f480f0-71e4-4d50-8efa-deae20172764",
    "outputId": "93d67e80-afea-4126-a194-c9168d7deb5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/redis-stack\" is already in use by container \"0f8431747c7706d69300b99a86c59aa98e9a7d7aeae2be59787afa9e42c90814\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "# if creating a new container\n",
    "!docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "# # if starting an existing container\n",
    "# !docker start -a redis-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63d3831-70e9-4b7b-b876-1143fd580c6c",
   "metadata": {
    "id": "b63d3831-70e9-4b7b-b876-1143fd580c6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-cf2vGKeLXFbUw702UEB0T3BlbkFJRJAY4TNZCAuErTbkbSsl' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0320e-47d6-48a8-9ba1-d844bb887cb5",
   "metadata": {
    "id": "69b0320e-47d6-48a8-9ba1-d844bb887cb5"
   },
   "source": [
    "## Define Ingestion Pipeline\n",
    "\n",
    "Here we define the ingestion pipeline. Given a set of documents, we will run sentence splitting/embedding transformations, and then load them into a Redis docstore/vector store.\n",
    "\n",
    "The vector store is for indexing the data + storing the embeddings, the docstore is for tracking duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2871ad-1c14-49e4-b5ec-3e3eb96429f8",
   "metadata": {
    "id": "1f2871ad-1c14-49e4-b5ec-3e3eb96429f8"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.ingestion import (\n",
    "    DocstoreStrategy,\n",
    "    IngestionPipeline,\n",
    "    IngestionCache,\n",
    ")\n",
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "from llama_index.storage.docstore.redis import RedisDocumentStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "\n",
    "from redisvl.schema import IndexSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf744be",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ac74203675564f14b73882a6ae270d18",
      "c93811def32744ce870253a77767777e",
      "8c237673c9ec4e22a4eba34c934cc322",
      "f66602de35274bb299d100783e73a01b",
      "7cc44d9f4fd84913b403a05124e71d9a",
      "f3a4992e06c44f2aac3f1a4d21e49065"
     ]
    },
    "id": "baf744be",
    "outputId": "12c2ba18-5efc-4353-cce5-48fd3935b722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emily/Desktop/code repos/log-analysis/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca63bf0d-9455-40cb-b30f-9a23e1990c08",
   "metadata": {
    "id": "ca63bf0d-9455-40cb-b30f-9a23e1990c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:13:48 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "# e: Define redis schema\n",
    "custom_schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\"name\": \"gdrive\", \"prefix\": \"doc\"},\n",
    "        # customize fields that are indexed\n",
    "        \"fields\": [\n",
    "            # required fields for llamaindex\n",
    "            {\"type\": \"tag\", \"name\": \"id\"},\n",
    "            {\"type\": \"tag\", \"name\": \"doc_id\"},\n",
    "            {\"type\": \"text\", \"name\": \"text\"},\n",
    "            # custom vector field for bge-small-en-v1.5 embeddings\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"name\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 384,\n",
    "                    \"algorithm\": \"hnsw\",\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# e: define vector store given schema\n",
    "vector_store = RedisVectorStore(\n",
    "    schema=custom_schema,\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78043d63-bd88-4367-b883-5ad6075339ca",
   "metadata": {
    "id": "78043d63-bd88-4367-b883-5ad6075339ca"
   },
   "outputs": [],
   "source": [
    "# Optional: clear vector store if exists\n",
    "if vector_store.index_exists():\n",
    "    vector_store.delete_index()\n",
    "vector_store.create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d98845",
   "metadata": {
    "id": "e6d98845"
   },
   "outputs": [],
   "source": [
    "# Set up the ingestion cache layer\n",
    "cache = IngestionCache(\n",
    "    cache=RedisCache.from_host_and_port(\"localhost\", 6379),\n",
    "    collection=\"redis_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45c58a1-6c86-445c-9275-be28bd1c25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = RedisDocumentStore.from_host_and_port(\n",
    "        \"localhost\", 6379, namespace=\"document_store\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be817bd-81a1-436f-8f92-3eb48531c915",
   "metadata": {
    "id": "3be817bd-81a1-436f-8f92-3eb48531c915"
   },
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embed_model,\n",
    "    ],\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    "    cache=cache,\n",
    "    docstore_strategy=DocstoreStrategy.UPSERTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a873168-f735-43cd-b511-0bb569f9c8b4",
   "metadata": {
    "id": "6a873168-f735-43cd-b511-0bb569f9c8b4"
   },
   "source": [
    "### Define our Vector Store Index\n",
    "\n",
    "We define our index to wrap the underlying vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5affc83-b5f0-40c9-a8a1-b4ddd67fa62b",
   "metadata": {
    "id": "d5affc83-b5f0-40c9-a8a1-b4ddd67fa62b"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    pipeline.vector_store, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f9de5-4373-458b-b6cf-a8173f3e9a52",
   "metadata": {
    "id": "343f9de5-4373-458b-b6cf-a8173f3e9a52"
   },
   "source": [
    "## Load Initial Data\n",
    "\n",
    "Here we load data from our [Google Drive Loader](https://llamahub.ai/l/readers/llama-index-readers-google?from=readers) on LlamaHub.\n",
    "\n",
    "The loaded docs are the header sections of our [Use Cases from our documentation](https://docs.llamaindex.ai/en/latest/use_cases/q_and_a/root.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a8bd50-c2a5-496e-a3d1-933483efd61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 docs\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "loader = SimpleDirectoryReader(\"data\")\n",
    "\n",
    "def load_data():\n",
    "    docs = loader.load_data()\n",
    "    print(len(docs), \"docs\")\n",
    "    for doc in docs:\n",
    "        doc.id_ = doc.metadata[\"file_name\"]\n",
    "    return docs\n",
    "docs = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77f74b2-9bbe-46d6-b35f-23ea757b315b",
   "metadata": {
    "id": "c77f74b2-9bbe-46d6-b35f-23ea757b315b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 0 Nodes\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=docs)\n",
    "print(f\"Ingested {len(nodes)} Nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae510add-f8d3-4fb3-a351-1cc7a5fe9e6b",
   "metadata": {
    "id": "ae510add-f8d3-4fb3-a351-1cc7a5fe9e6b"
   },
   "source": [
    "Since this is our first time starting up the vector store, we see that we've transformed/ingested all the documents into it (by chunking, and then by embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687636-45e3-4038-b72e-b8c2d86baf56",
   "metadata": {
    "id": "f9687636-45e3-4038-b72e-b8c2d86baf56"
   },
   "source": [
    "### Ask Questions over Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "49133858-de8f-4cbe-bc83-c951c5bbe7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt to use Pydantic Output response formats\n",
    "# from typing import List\n",
    "# from pydantic import BaseModel\n",
    "\n",
    "# class Program(BaseModel):\n",
    "#     \"\"\"Data model for python program.\"\"\"\n",
    "\n",
    "#     name: str\n",
    "#     python_code : str\n",
    "#     explanation : str\n",
    "\n",
    "# query_engine = index.as_query_engine(llm=llm, response_model=\"refine\", output_cls=Program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cdb1fdc-af39-4afd-9629-78b54dc5fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output parseroutput_parser <llama_index.core.output_parsers.langchain.LangchainOutputParser object at 0x180587c50>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.output_parsers import LangchainOutputParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# define output schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"Python code\",\n",
    "        description=\"Write python code that correspond to the query\",\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name=\"Explanation\",\n",
    "        description=\"Describe what you've done\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# define output parser\n",
    "lc_output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    response_schemas\n",
    ")\n",
    "output_parser = LangchainOutputParser(lc_output_parser)\n",
    "print(\"Output parseroutput_parser\", output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003675b4-0776-4142-a959-e4753c115d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1, model=\"gpt-4o\", output_parser=output_parser)\n",
    "# obtain a structured response\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "# response = query_engine.query(\n",
    "#     \"What are a few things the author did growing up?\",\n",
    "# )\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65ddcc68-c426-40f4-ac3b-16495855e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python code': '', 'Explanation': 'The documentation is about the Ursina Engine, which includes tutorials, installation guides, and references for various features such as entity basics, coordinate systems, collisions, and text handling.'}\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is this documentation about?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad8e178-60fd-4405-b74f-0fdbd7b0ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python code': \"from ursina import *\\n\\nif __name__ == '__main__':\\n    app = Ursina()\\n\\ncamera.orthographic = True\\ncamera.fov = 4\\ncamera.position = (1, 1)\\nText.default_resolution *= 2\\n\\nplayer = Entity(name='o', color=color.azure)\\ncursor = Tooltip(player.name, color=player.color, origin=(0,0), scale=4, enabled=True)\\ncursor.background.color = color.clear\\nbg = Entity(parent=scene, model='quad', texture='shore', scale=(16,8), z=10, color=color.light_gray)\\nmouse.visible = False\\n\\n# create a matrix to store the buttons in. makes it easier to check for victory\\nboard = [[None for x in range(3)] for y in range(3)]\\n\\nfor y in range(3):\\n    for x in range(3):\\n        b = Button(parent=scene, position=(x,y))\\n        board[x][y] = b\\n\\n        def on_click(b=b):\\n            b.text = player.name\\n            b.color = player.color\\n            b.collision = False\\n            check_for_victory()\\n\\n            if player.name == 'o':\\n                player.name = 'x'\\n                player.color = color.orange\\n            else:\\n                player.name = 'o'\\n                player.color = color.azure\\n\\n            cursor.text = player.name\\n            cursor.color = player.color\\n\\n        b.on_click = on_click\\n\\n\\ndef check_for_victory():\\n    name = player.name\\n\\n    won = (\\n    (board[0][0].text == name and board[1][0].text == name and board[2][0].text == name) or # across the bottom\\n    (board[0][1].text == name and board[1][1].text == name and board[2][1].text == name) or # across the middle\\n    (board[0][2].text == name and board[1][2].text == name and board[2][2].text == name) or # across the top\\n    (board[0][0].text == name and board[0][1].text == name and board[0][2].text == name) or # down the left side\\n    (board[1][0].text == name and board[1][1].text == name and board[1][2].text == name) or # down the middle\\n    (board[2][0].text == name and board[2][1].text == name and board[2][2].text == name) or # down the right side\\n    (board[0][0].text == name and board[1][1].text == name and board[2][2].text == name) or # diagonal /\\n    (board[0][2].text == name and board[1][1].text == name and board[2][0].text == name))   # diagonal \\\\\\n\\n    if won:\\n        print('winner is:', name)\\n        cursor.text = ''\\n        mouse.visible = True\\n        Panel(z=1, scale=10, model='quad')\\n        t = Text(f'player\\\\n{name}\\\\nwon!', scale=3, origin=(0,0), background=True)\\n        t.create_background(padding=(.5,.25), radius=Text.size/2)\\n        t.background.color = player.color.tint(-.2)\\n\\nif __name__ == '__main__':\\n    app.run()\", 'Explanation': \"The provided Python code creates a Tic Tac Toe game using the Ursina engine. It sets up the game window, initializes the camera, and creates a 3x3 grid of buttons representing the game board. Players take turns clicking the buttons to place their marks ('o' or 'x'). The game checks for a victory condition after each move and announces the winner if a player wins.\"}\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Write a tic tac toe game\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7834cf-890c-49a6-b41b-c27698ceabe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Code:\n",
      " from ursina import *\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app = Ursina()\n",
      "\n",
      "camera.orthographic = True\n",
      "camera.fov = 4\n",
      "camera.position = (1, 1)\n",
      "Text.default_resolution *= 2\n",
      "\n",
      "player = Entity(name='o', color=color.azure)\n",
      "cursor = Tooltip(player.name, color=player.color, origin=(0,0), scale=4, enabled=True)\n",
      "cursor.background.color = color.clear\n",
      "bg = Entity(parent=scene, model='quad', texture='shore', scale=(16,8), z=10, color=color.light_gray)\n",
      "mouse.visible = False\n",
      "\n",
      "# create a matrix to store the buttons in. makes it easier to check for victory\n",
      "board = [[None for x in range(3)] for y in range(3)]\n",
      "\n",
      "for y in range(3):\n",
      "    for x in range(3):\n",
      "        b = Button(parent=scene, position=(x,y))\n",
      "        board[x][y] = b\n",
      "\n",
      "        def on_click(b=b):\n",
      "            b.text = player.name\n",
      "            b.color = player.color\n",
      "            b.collision = False\n",
      "            check_for_victory()\n",
      "\n",
      "            if player.name == 'o':\n",
      "                player.name = 'x'\n",
      "                player.color = color.orange\n",
      "            else:\n",
      "                player.name = 'o'\n",
      "                player.color = color.azure\n",
      "\n",
      "            cursor.text = player.name\n",
      "            cursor.color = player.color\n",
      "\n",
      "        b.on_click = on_click\n",
      "\n",
      "\n",
      "def check_for_victory():\n",
      "    name = player.name\n",
      "\n",
      "    won = (\n",
      "    (board[0][0].text == name and board[1][0].text == name and board[2][0].text == name) or # across the bottom\n",
      "    (board[0][1].text == name and board[1][1].text == name and board[2][1].text == name) or # across the middle\n",
      "    (board[0][2].text == name and board[1][2].text == name and board[2][2].text == name) or # across the top\n",
      "    (board[0][0].text == name and board[0][1].text == name and board[0][2].text == name) or # down the left side\n",
      "    (board[1][0].text == name and board[1][1].text == name and board[1][2].text == name) or # down the middle\n",
      "    (board[2][0].text == name and board[2][1].text == name and board[2][2].text == name) or # down the right side\n",
      "    (board[0][0].text == name and board[1][1].text == name and board[2][2].text == name) or # diagonal /\n",
      "    (board[0][2].text == name and board[1][1].text == name and board[2][0].text == name))   # diagonal \\\n",
      "\n",
      "    if won:\n",
      "        print('winner is:', name)\n",
      "        cursor.text = ''\n",
      "        mouse.visible = True\n",
      "        Panel(z=1, scale=10, model='quad')\n",
      "        t = Text(f'player\\n{name}\\nwon!', scale=3, origin=(0,0), background=True)\n",
      "        t.create_background(padding=(.5,.25), radius=Text.size/2)\n",
      "        t.background.color = player.color.tint(-.2)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "\n",
      "Explanation:\n",
      " The provided Python code creates a Tic Tac Toe game using the Ursina engine. It sets up the game window, initializes the camera, and creates a 3x3 grid of buttons representing the game board. Players take turns clicking the buttons to place their marks ('o' or 'x'). The game checks for a victory condition after each move and announces the winner if a player wins.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Assuming response_obj is your Response object\n",
    "response_str = response.response\n",
    "\n",
    "# Parse the string representation of the dictionary\n",
    "response_dict = ast.literal_eval(response_str)\n",
    "\n",
    "# Extract 'Python code' and 'Explanation'\n",
    "python_code = response_dict.get('Python code')\n",
    "explanation = response_dict.get('Explanation')\n",
    "\n",
    "print(\"Python Code:\\n\", python_code)\n",
    "print(\"\\nExplanation:\\n\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69e2c4-3ff1-40dc-b07b-17307d53ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## product\n",
    "# chat with your documentation\n",
    "# iteratively edit code?\n",
    "\n",
    "\n",
    "## todos\n",
    "# turn the above local file upload / enter path into chatbot & pipe it into UI to show dad\n",
    "# execute the output code\n",
    "# documentation parsing into structured pydantic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03495a-7c01-4530-a12d-feb03f26abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the examples provided here?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b5201-0b69-4419-9294-c03ee85b0755",
   "metadata": {
    "id": "800b5201-0b69-4419-9294-c03ee85b0755"
   },
   "source": [
    "## Modify and Reload the Data\n",
    "\n",
    "Let's try modifying our ingested data!\n",
    "\n",
    "We modify the \"Q&A\" doc to include an extra \"structured analytics\" block of text. See our [updated document](https://docs.google.com/document/d/1QQMKNAgyplv2IUOKNClEBymOFaASwmsZFoLmO_IeSTw/edit?usp=sharing) as a reference.\n",
    "\n",
    "Now let's rerun the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490fbb8-82ec-4284-a19d-1a8ca69da2a4",
   "metadata": {
    "id": "d490fbb8-82ec-4284-a19d-1a8ca69da2a4"
   },
   "outputs": [],
   "source": [
    "# docs = load_data(folder_id=\"1RFhr3-KmOZCR5rtp4dlOMNl3LKe1kOA5\")\n",
    "# nodes = pipeline.run(documents=docs)\n",
    "# print(f\"Ingested {len(nodes)} Nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768505db-02ee-4929-a8e7-1ee127356c98",
   "metadata": {
    "id": "768505db-02ee-4929-a8e7-1ee127356c98"
   },
   "source": [
    "Notice how only one node is ingested. This is beacuse only one document changed, while the other documents stayed the same. This means that we only need to re-transform and re-embed one document!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba5205-09d7-46f0-8a97-ac58c3f9b649",
   "metadata": {
    "id": "56ba5205-09d7-46f0-8a97-ac58c3f9b649"
   },
   "source": [
    "### Ask Questions over New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d5f4b-7818-437b-ac33-ce8257e00048",
   "metadata": {
    "id": "b52d5f4b-7818-437b-ac33-ce8257e00048"
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf1b0b-f6f1-45eb-ac61-a154a66c57d7",
   "metadata": {
    "id": "b5cf1b0b-f6f1-45eb-ac61-a154a66c57d7"
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the sub-types of question answering?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cce9e-567a-4ef4-8793-875880e09756",
   "metadata": {
    "id": "486cce9e-567a-4ef4-8793-875880e09756",
    "outputId": "a4aba8c9-9a29-4ef8-8c4e-78778b3be90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sub-types of question answering mentioned in the context are semantic search, summarization, and structured analytics.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
